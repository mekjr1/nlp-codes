{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport glob\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-08-24T18:56:36.863317Z","iopub.execute_input":"2022-08-24T18:56:36.864098Z","iopub.status.idle":"2022-08-24T18:56:36.913369Z","shell.execute_reply.started":"2022-08-24T18:56:36.863968Z","shell.execute_reply":"2022-08-24T18:56:36.912116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**General Functions**","metadata":{}},{"cell_type":"code","source":"def remove_retweets():\n    # checking if the tweet is a retweet (this method is basic but it will work)\n    if tweettext.startswith(\"rt @\") == True:\n        print('This tweet is a retweet')\n    else:\n        print('This tweet is not retweet')","metadata":{"execution":{"iopub.status.busy":"2022-08-23T23:10:45.604019Z","iopub.execute_input":"2022-08-23T23:10:45.604892Z","iopub.status.idle":"2022-08-23T23:10:45.611692Z","shell.execute_reply.started":"2022-08-23T23:10:45.604834Z","shell.execute_reply":"2022-08-23T23:10:45.610521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Cleaning and Sampling Reddit Data**","metadata":{}},{"cell_type":"markdown","source":"Reading the Reddit input files","metadata":{}},{"cell_type":"code","source":"extension = 'csv' \nall_filenames = [i for i in glob.glob('../input/raw-guilt-data-from-reddit/*.{}'.format(extension))]","metadata":{"execution":{"iopub.status.busy":"2022-08-23T23:10:45.613287Z","iopub.execute_input":"2022-08-23T23:10:45.613892Z","iopub.status.idle":"2022-08-23T23:10:45.627261Z","shell.execute_reply.started":"2022-08-23T23:10:45.613775Z","shell.execute_reply":"2022-08-23T23:10:45.626117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Remove all unusable rows, and raws with word count outside the bounds 40-100, processing one file at a time","metadata":{}},{"cell_type":"code","source":"reddit_data = []\n\nfor infile in all_filenames:\n    print(\"Now processing file: \", infile)\n    data = pd.read_csv(infile, engine='python')\n    print(\"Initial Size:\", data.shape)\n    data.dropna(inplace = True)\n    print(\"After Droping null Size:\", data.shape)\n    data.drop_duplicates(inplace=True)\n    print(\"After Droping Duplicates Size:\", data.shape)\n    data['w_counts']=data['selftext'].apply(lambda x: len(str(x).split(' ')))\n    data = data[data[\"selftext\"] != \"[removed]\"]\n    print(\"After Droping [removed] Size:\", data.shape)\n    data = data[data[\"selftext\"] != \"[deleted]\"]\n    print(\"After Droping [deleted] Size:\", data.shape)\n    data = data[data[\"selftext\"] != \"deleted\"]\n    print(\"After Droping 'deleted' Size:\", data.shape)\n    data = data[data[\"selftext\"] != \"removed\"]\n    print(\"After Droping 'removed' Size:\", data.shape)\n    data=data[data[\"w_counts\"]>40]\n    print(\"After Droping less than 40 Size:\", data.shape)\n    data=data[data[\"w_counts\"]<100]\n    print(\"After Droping more than 100 Size:\", data.shape)\n    reddit_data.append(data)         \n    print(\"Final Size:\", data.shape)\n    \n    \n    # store DataFrame in list\nreddit_data= pd.concat(reddit_data)    \nreddit_data.to_csv(\"reddit_submissions_partially_cleaned_final.csv\", index=False, encoding='utf-8-sig')","metadata":{"execution":{"iopub.status.busy":"2022-08-23T23:10:45.628757Z","iopub.execute_input":"2022-08-23T23:10:45.629914Z","iopub.status.idle":"2022-08-23T23:35:39.206598Z","shell.execute_reply.started":"2022-08-23T23:10:45.629865Z","shell.execute_reply":"2022-08-23T23:35:39.205139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#all_filenames = [i for i in glob.glob('*.{}'.format(extension))]\n\n#df = pd.concat(map(pd.read_csv, all_filenames), ignore_index=True) \n\n#df.to_csv( \"combined_confession_keywords.csv\", index=False, encoding='utf-8-sig')","metadata":{"execution":{"iopub.status.busy":"2022-08-23T23:35:39.213347Z","iopub.execute_input":"2022-08-23T23:35:39.214186Z","iopub.status.idle":"2022-08-23T23:36:16.871895Z","shell.execute_reply.started":"2022-08-23T23:35:39.214129Z","shell.execute_reply":"2022-08-23T23:36:16.870330Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_sample=reddit_data.sample(n=12000)\nfinal_sample.to_csv( \"12k_sample_reddit_cleaned_final_to_annotate.csv\", index=False, encoding='utf-8-sig')","metadata":{"execution":{"iopub.status.busy":"2022-08-23T23:36:16.874677Z","iopub.execute_input":"2022-08-23T23:36:16.875099Z","iopub.status.idle":"2022-08-23T23:36:17.205981Z","shell.execute_reply.started":"2022-08-23T23:36:16.875060Z","shell.execute_reply":"2022-08-23T23:36:17.204744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Cleaning and Sampling Twitter Data**","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(\"../input/raw-guilt-data-from-twitter/complete_raw_twitter_tweets.csv\", engine='python')","metadata":{"execution":{"iopub.status.busy":"2022-08-24T19:09:55.694799Z","iopub.execute_input":"2022-08-24T19:09:55.695279Z","iopub.status.idle":"2022-08-24T19:10:02.754635Z","shell.execute_reply.started":"2022-08-24T19:09:55.695238Z","shell.execute_reply":"2022-08-24T19:10:02.753123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.groupby(by=[\"lang\"]).count()\n\nprint('Dataset size before cleaning:',data.shape)\nprint('Columns are:',data.columns)\nprint(data.info())","metadata":{"execution":{"iopub.status.busy":"2022-08-23T23:36:24.642675Z","iopub.execute_input":"2022-08-23T23:36:24.643106Z","iopub.status.idle":"2022-08-23T23:36:25.711101Z","shell.execute_reply.started":"2022-08-23T23:36:24.643073Z","shell.execute_reply":"2022-08-23T23:36:25.709829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def is_retweet(tweettext):\n    # checking if the tweet is a retweet (this method is basic but it will work)\n    return tweettext.startswith((\"rt @\") )","metadata":{"execution":{"iopub.status.busy":"2022-08-24T19:08:16.697010Z","iopub.execute_input":"2022-08-24T19:08:16.697495Z","iopub.status.idle":"2022-08-24T19:08:16.702797Z","shell.execute_reply.started":"2022-08-24T19:08:16.697457Z","shell.execute_reply":"2022-08-24T19:08:16.701860Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(data.head(20))","metadata":{"execution":{"iopub.status.busy":"2022-08-24T19:04:01.159076Z","iopub.execute_input":"2022-08-24T19:04:01.159540Z","iopub.status.idle":"2022-08-24T19:04:01.173515Z","shell.execute_reply.started":"2022-08-24T19:04:01.159501Z","shell.execute_reply":"2022-08-24T19:04:01.172345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Initial Size:\", data.shape)\ndata.dropna(subset = ['tweet'], inplace = True)\nprint(\"After Droping null Size:\", data.shape)\ndata.drop_duplicates(subset = ['id'],inplace=True)\nprint(\"After Droping Duplicates Size:\", data.shape)\ndata['is_retweet']=data['tweet'].apply(lambda x: is_retweet(x))\nprint(data.head(20))\ndata = data[data[\"is_retweet\"] != True]\nprint(\"After Droping Retweets Size:\", data.shape)\ndata = data[data[\"lang\"] == \"en\"]\nprint(\"After Droping non-english Size:\", data.shape)\nprint(data.head(20))\n    \nprint(\"Final Size:\", data.shape)","metadata":{"execution":{"iopub.status.busy":"2022-08-24T19:10:06.968692Z","iopub.execute_input":"2022-08-24T19:10:06.969152Z","iopub.status.idle":"2022-08-24T19:10:07.881273Z","shell.execute_reply.started":"2022-08-24T19:10:06.969085Z","shell.execute_reply":"2022-08-24T19:10:07.880207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"twitter_data=data.sample(n=12000)\ntwitter_data.to_csv( \"12k_sample_twitter_cleaned_final_to_annotate.csv\", index=False, encoding='utf-8-sig')","metadata":{"execution":{"iopub.status.busy":"2022-08-23T23:36:26.540706Z","iopub.execute_input":"2022-08-23T23:36:26.541494Z","iopub.status.idle":"2022-08-23T23:36:26.590233Z","shell.execute_reply.started":"2022-08-23T23:36:26.541458Z","shell.execute_reply":"2022-08-23T23:36:26.588063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}