{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers\nimport logging\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport transformers\nimport os\nfrom sklearn.model_selection import train_test_split\n\n\nlogging.disable(logging.INFO) # disable INFO and DEBUG logging everywhere\n# or \nlogging.disable(logging.WARNING) # disable WARNING, INFO and DEBUG logging everywhere\n\n\nmax_length = 128  # Maximum length of input sentence to the model.\nbatch_size = 32\nepochs = 2\n\n# Labels in our dataset.\nlabels = [0,1]","metadata":{"id":"_KNYVcutuAQP","outputId":"5f253c4c-4269-4f18-a252-0e6950ae12e0","execution":{"iopub.status.busy":"2022-03-30T21:53:37.009013Z","iopub.execute_input":"2022-03-30T21:53:37.009354Z","iopub.status.idle":"2022-03-30T21:53:51.976552Z","shell.execute_reply.started":"2022-03-30T21:53:37.009267Z","shell.execute_reply":"2022-03-30T21:53:51.975829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_dataset():\n\n    train = pd.read_csv('https://raw.githubusercontent.com/GIL-UNAM/PARMEX_2022/main/parmex_train.csv')\n    valid = pd.read_csv('https://raw.githubusercontent.com/GIL-UNAM/PARMEX_2022/main/parmex_validation.csv')\n    print(\"Train Dataframe:\")\n    train.head(3)\n    print(f'Train dataframe contains {train.shape[0]} samples.')\n    print('Number of features in train data : ', train.shape[1])\n    print('Train Features : ', train.columns.values)\n\n    train_df,test_df=train_test_split(train,test_size=0.25,random_state=10)\n\n    valid_df=test_df\n\n    train_df = (\n        train_df[train_df.Label != \"-\"]\n        .sample(frac=1.0, random_state=42)\n        .reset_index(drop=True)\n    )\n    valid_df = (\n        valid_df[valid_df.Label != \"-\"]\n        .sample(frac=1.0, random_state=42)\n        .reset_index(drop=True)\n    )\n\n    y_train = tf.keras.utils.to_categorical(train_df.Label, num_classes=2)\n\n\n    y_val = tf.keras.utils.to_categorical(valid_df.Label, num_classes=2)\n\n\n    y_test = tf.keras.utils.to_categorical(test_df.Label, num_classes=2)\n\n    return train_df, valid_df, valid_df, y_train, y_val, y_test\n","metadata":{"id":"e1zlDQNkur-5","execution":{"iopub.status.busy":"2022-03-30T21:53:51.979895Z","iopub.execute_input":"2022-03-30T21:53:51.98011Z","iopub.status.idle":"2022-03-30T21:53:51.989721Z","shell.execute_reply.started":"2022-03-30T21:53:51.980082Z","shell.execute_reply":"2022-03-30T21:53:51.987921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nclass BertSemanticDataGenerator(tf.keras.utils.Sequence):\n    \"\"\"Generates batches of data.\n\n    Args:\n        sentence_pairs: Array of premise and hypothesis input sentences.\n        labels: Array of labels.\n        batch_size: Integer batch size.\n        shuffle: boolean, whether to shuffle the data.\n        include_targets: boolean, whether to incude the labels.\n\n    Returns:\n        Tuples `([input_ids, attention_mask, `token_type_ids], labels)`\n        (or just `[input_ids, attention_mask, `token_type_ids]`\n         if `include_targets=False`)\n    \"\"\"\n\n    def __init__(\n        self,\n        sentence_pairs,\n        labels,\n        batch_size=batch_size,\n        shuffle=True,\n        include_targets=True,\n        model_path=''\n    ):\n        self.sentence_pairs = sentence_pairs\n        self.labels = labels\n        self.shuffle = shuffle\n        self.batch_size = batch_size\n        self.include_targets = include_targets\n        self.model_path=model_path\n        # Load our BERT Tokenizer to encode the text.\n        # We will use base-base-uncased pretrained model.\n        self.tokenizer = transformers.BertTokenizer.from_pretrained(\n            model_path, do_lower_case=True##################################################################################################\n        )\n        self.indexes = np.arange(len(self.sentence_pairs))\n        self.on_epoch_end()\n\n    def __len__(self):\n        # Denotes the number of batches per epoch.\n        return len(self.sentence_pairs) // self.batch_size\n\n    def __getitem__(self, idx):\n        # Retrieves the batch of index.\n        indexes = self.indexes[idx * self.batch_size : (idx + 1) * self.batch_size]\n        sentence_pairs = self.sentence_pairs[indexes]\n\n        # With BERT tokenizer's batch_encode_plus batch of both the sentences are\n        # encoded together and separated by [SEP] token.\n        encoded = self.tokenizer.batch_encode_plus(\n            sentence_pairs.tolist(),\n            add_special_tokens=True,\n            max_length=max_length,\n            return_attention_mask=True,\n            return_token_type_ids=True,\n            pad_to_max_length=True,\n            return_tensors=\"tf\",\n        )\n\n        # Convert batch of encoded features to numpy array.\n        input_ids = np.array(encoded[\"input_ids\"], dtype=\"int32\")\n        attention_masks = np.array(encoded[\"attention_mask\"], dtype=\"int32\")\n        token_type_ids = np.array(encoded[\"token_type_ids\"], dtype=\"int32\")\n\n        # Set to true if data generator is used for training/validation.\n        if self.include_targets:\n            labels = np.array(self.labels[indexes], dtype=\"int32\")\n            return [input_ids, attention_masks, token_type_ids], labels\n        else:\n            return [input_ids, attention_masks, token_type_ids]\n\n    def on_epoch_end(self):\n        # Shuffle indexes after each epoch if shuffle is set to True.\n        if self.shuffle:\n            np.random.RandomState(42).shuffle(self.indexes)","metadata":{"id":"QFExndJyuuwR","execution":{"iopub.status.busy":"2022-03-30T21:53:51.991427Z","iopub.execute_input":"2022-03-30T21:53:51.991735Z","iopub.status.idle":"2022-03-30T21:53:52.962322Z","shell.execute_reply.started":"2022-03-30T21:53:51.991702Z","shell.execute_reply":"2022-03-30T21:53:52.96144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the model under a distribution strategy scope.\ndef create_model(model_path):\n    strategy = tf.distribute.MirroredStrategy()\n\n    with strategy.scope():\n        # Encoded token ids from BERT tokenizer.\n        input_ids = tf.keras.layers.Input(\n            shape=(max_length,), dtype=tf.int32, name=\"input_ids\"\n        )\n        # Attention masks indicates to the model which tokens should be attended to.\n        attention_masks = tf.keras.layers.Input(\n            shape=(max_length,), dtype=tf.int32, name=\"attention_masks\"\n        )\n        # Token type ids are binary masks identifying different sequences in the model.\n        token_type_ids = tf.keras.layers.Input(\n            shape=(max_length,), dtype=tf.int32, name=\"token_type_ids\"\n        )\n        # Loading pretrained BERT model.\n        bert_model = transformers.TFBertModel.from_pretrained(model_path)########################################################################\n        # Freeze the BERT model to reuse the pretrained features without modifying them.\n        bert_model.trainable = False\n\n        bert_output = bert_model(\n            input_ids, attention_mask=attention_masks, token_type_ids=token_type_ids\n        )\n        sequence_output = bert_output.last_hidden_state\n        pooled_output = bert_output.pooler_output\n        # Add trainable layers on top of frozen layers to adapt the pretrained features on the new data.\n        bi_lstm = tf.keras.layers.Bidirectional(\n            tf.keras.layers.LSTM(64, return_sequences=True)\n        )(sequence_output)\n        # Applying hybrid pooling approach to bi_lstm sequence output.\n        avg_pool = tf.keras.layers.GlobalAveragePooling1D()(bi_lstm)\n        max_pool = tf.keras.layers.GlobalMaxPooling1D()(bi_lstm)\n        concat = tf.keras.layers.concatenate([avg_pool, max_pool])\n        dropout = tf.keras.layers.Dropout(0.3)(concat)\n        output = tf.keras.layers.Dense(2, activation=\"softmax\")(dropout)\n        model = tf.keras.models.Model(\n            inputs=[input_ids, attention_masks, token_type_ids], outputs=output\n        )\n\n        model.compile(\n            optimizer=tf.keras.optimizers.Adam(),\n            loss=\"categorical_crossentropy\",\n            metrics=[\"acc\"],\n        )\n\n\n    print(f\"Strategy: {strategy}\")\n    model.summary()\n\n    return model, bert_model\n","metadata":{"id":"BNsuU1SHumJF","execution":{"iopub.status.busy":"2022-03-30T21:53:52.964337Z","iopub.execute_input":"2022-03-30T21:53:52.964994Z","iopub.status.idle":"2022-03-30T21:53:52.983282Z","shell.execute_reply.started":"2022-03-30T21:53:52.964958Z","shell.execute_reply":"2022-03-30T21:53:52.982689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_bert_semantic_data(data_set, text1, text2, y, model_path):\n    semantic_data = BertSemanticDataGenerator(\n    data_set[[text1, text2]].values.astype(\"str\"),\n    y,\n    batch_size=batch_size,\n    shuffle=True,\n    model_path=model_path\n    )\n\n\n    return semantic_data\n\n\n\n","metadata":{"id":"vjsV5vuzubhV","execution":{"iopub.status.busy":"2022-03-30T21:53:52.984581Z","iopub.execute_input":"2022-03-30T21:53:52.985023Z","iopub.status.idle":"2022-03-30T21:53:52.997894Z","shell.execute_reply.started":"2022-03-30T21:53:52.984987Z","shell.execute_reply":"2022-03-30T21:53:52.99704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fit_module(model, train, valid, epochs):\n\n    history = model.fit(\n    train,\n    validation_data=valid,\n    epochs=epochs,\n    use_multiprocessing=True,\n    workers=-1,\n    )\n\n    return history, model\n","metadata":{"id":"GbJQUyVmudqp","execution":{"iopub.status.busy":"2022-03-30T21:53:52.999514Z","iopub.execute_input":"2022-03-30T21:53:52.999888Z","iopub.status.idle":"2022-03-30T21:53:53.007177Z","shell.execute_reply.started":"2022-03-30T21:53:52.999851Z","shell.execute_reply":"2022-03-30T21:53:53.005826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def  unfreeze_recompile_model(bert_model, model):\n    bert_model.trainable = True\n\n    model.compile(\n    optimizer=tf.keras.optimizers.Adam(1e-5),\n    loss=\"categorical_crossentropy\",\n    metrics=[\"accuracy\"],\n    )\n    model.summary()\n\n    return model.summary(), model\n","metadata":{"id":"aWTzp1eSuNRN","execution":{"iopub.status.busy":"2022-03-30T21:53:53.008804Z","iopub.execute_input":"2022-03-30T21:53:53.009596Z","iopub.status.idle":"2022-03-30T21:53:53.018925Z","shell.execute_reply.started":"2022-03-30T21:53:53.009559Z","shell.execute_reply":"2022-03-30T21:53:53.018207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef fit_model(model, train, valid, epochs):\n    history = model.fit(\n    train_data,\n    validation_data=valid_data,\n    epochs=epochs,\n    use_multiprocessing=True,\n    workers=-1,\n    )\n\n    return history, model\n\n","metadata":{"id":"T_cSZLmcuNsI","execution":{"iopub.status.busy":"2022-03-30T21:53:53.020079Z","iopub.execute_input":"2022-03-30T21:53:53.020477Z","iopub.status.idle":"2022-03-30T21:53:53.028705Z","shell.execute_reply.started":"2022-03-30T21:53:53.020442Z","shell.execute_reply":"2022-03-30T21:53:53.028031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def validate_model(model, test_data, verbose=1):\n\n    return model.evaluate(test_data,verbose)\n\n\n\n","metadata":{"id":"siwqt3ojuTO0","execution":{"iopub.status.busy":"2022-03-30T21:53:53.029867Z","iopub.execute_input":"2022-03-30T21:53:53.030769Z","iopub.status.idle":"2022-03-30T21:53:53.040003Z","shell.execute_reply.started":"2022-03-30T21:53:53.03073Z","shell.execute_reply":"2022-03-30T21:53:53.039095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def check_similarity(model_path ,sentence1, sentence2):\n    sentence_pairs = np.array([[str(sentence1), str(sentence2)]])\n    test_data = BertSemanticDataGenerator(\n        sentence_pairs, labels=None, batch_size=1, shuffle=False, include_targets=False,model_path=model_path\n    )\n\n    proba = model.predict(test_data[0])[0]\n    idx = np.argmax(proba)\n    proba = f\"{proba[idx]: .2f}%\"\n    pred = labels[idx]\n    return pred, proba","metadata":{"id":"T_CRJLhguVDN","execution":{"iopub.status.busy":"2022-03-30T21:53:53.042604Z","iopub.execute_input":"2022-03-30T21:53:53.043447Z","iopub.status.idle":"2022-03-30T21:53:53.060045Z","shell.execute_reply.started":"2022-03-30T21:53:53.043341Z","shell.execute_reply":"2022-03-30T21:53:53.059143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{"id":"bGe7xmyduICE"}},{"cell_type":"code","source":"model_list  =['dccuchile/bert-base-spanish-wwm-cased','hiiamsid/sentence_similarity_spanish_es','sentence-transformers/bert-base-nli-mean-tokens']\n\n\nprint ('here now')\n\n\n\n\ntokenizer=''\nbert_model=''\nmax_length = 128  # Maximum length of input sentence to the model.\nbatch_size = 32\nepochs = 2\n\nvalid = pd.read_csv('https://raw.githubusercontent.com/GIL-UNAM/PARMEX_2022/main/parmex_validation.csv')\n\nfor modx in model_list:\n    print(\"now doing this model\")\n    print(modx)\n    model_path = '/tokenizer2/'+modx\n    bert_model = transformers.TFBertModel.from_pretrained(modx,from_pt=True)\n    bert_model.save_pretrained(model_path)\n    tokenizer = transformers.BertTokenizer.from_pretrained(modx)\n    tokenizer.save_pretrained(model_path)\n\n    model, bert_model =create_model(model_path)\n    train_df, valid_df, test_df, y_train, y_val, y_test = prepare_dataset()\n\n    train_data = create_bert_semantic_data(train_df, \"Text1\", \"Text2\", y_train, model_path=model_path)\n\n    valid_data = create_bert_semantic_data(valid_df, \"Text1\", \"Text2\", y_train,model_path=model_path)\n\n    test_data = create_bert_semantic_data(test_df, \"Text1\", \"Text2\", y_train, model_path=model_path)\n    \n\n    history, model = fit_model(model, train_data, valid_data, epochs)\n\n    history,model = unfreeze_recompile_model(bert_model, model)\n\n    history, model = fit_model(model,train_data, valid_data,epochs)\n\n\n    predictionst = np.argmax(model.predict(test_data), axis=-1)\n    \n    \n    \n    ppp=[]\n    for i in range(len(train_df)):\n        \n        ppp.append(check_similarity(model_path,train_df['Text1'][i], train_df['Text2'][i])[0])\n    \n    train_df['predictions'] = ppp\n    train_df.to_csv(modx.split(\"/\")[1]+'misclassifyTrain.csv', header=True, index=False, columns=list(train_df.axes[1]))\n    \n    ppp=[]\n    for i in range(len(test_df)):\n        ppp.append(check_similarity(model_path,test_df['Text1'][i], test_df['Text2'][i])[0])\n    test_df['predictions'] = ppp\n    test_df.to_csv(modx.split(\"/\")[1]+'misclassifyTest.csv', header=True, index=False, columns=list(test_df.axes[1]))\n    \n    ppp=[]\n    for i in range(len(valid)):\n        ppp.append(check_similarity(model_path,valid['Text1'][i], valid['Text2'][i])[0])\n    valid['predictions'] = ppp\n    valid.to_csv(modx.split(\"/\")[1]+'misclassifyTest.csv', header=True, index=False, columns=list(valid.axes[1]))\n    \n    \n\n    '''h_df = pd.DataFrame(predictionst)\n    hist_csv_file = modx+'history.csv'\n    with open(hist_csv_file, mode='w') as f:\n        h_df.to_csv(f)\n    \n    predictionsv  = np.argmax(model.predict(test_data), axis=-1)\n\n    h_df = pd.DataFrame(predictionsv)\n    hist_csv_file = modx+'historyvalid.csv'\n    with open(hist_csv_file, mode='w') as f:\n        h_df.to_csv(f)\n    '''\n    \n\n\n\n\n    \n\n\n\n\n\n\n","metadata":{"id":"YyY9nFskuIMv","execution":{"iopub.status.busy":"2022-03-30T21:53:53.061825Z","iopub.execute_input":"2022-03-30T21:53:53.062362Z"},"trusted":true},"execution_count":null,"outputs":[]}]}