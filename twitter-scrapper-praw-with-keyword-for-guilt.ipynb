{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# For sending GET requests from the API\nimport requests\n# For saving access tokens and for file management when creating and adding to the dataset\nimport os\n# For dealing with json responses we receive from the API\nimport json\n# For displaying the data after\n\nimport csv\n# For parsing the dates received from twitter in readable formats\nimport datetime\nimport dateutil.parser\nimport unicodedata\n#To add wait time between requests\nimport time\n\n\nfrom datetime import datetime as dt\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-08-22T21:25:14.026163Z","iopub.execute_input":"2022-08-22T21:25:14.026650Z","iopub.status.idle":"2022-08-22T21:25:14.035181Z","shell.execute_reply.started":"2022-08-22T21:25:14.026614Z","shell.execute_reply":"2022-08-22T21:25:14.034154Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"os.environ['TwitterBearer'] = 'AAAAAAAAAAAAAAAAAAAAAOTlYgEAAAAAN6NQtvxLs8w6uYjCKHyus0P2qbQ%3DeZBReuwEU9IGT2DEbLrB0DYyd2xNc3oPND6amo5wGmTbKUTT1v'\n","metadata":{"execution":{"iopub.status.busy":"2022-08-22T21:25:14.037255Z","iopub.execute_input":"2022-08-22T21:25:14.037913Z","iopub.status.idle":"2022-08-22T21:25:14.049030Z","shell.execute_reply.started":"2022-08-22T21:25:14.037881Z","shell.execute_reply":"2022-08-22T21:25:14.047906Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def auth():\n    return os.getenv('TwitterBearer')\n\n\ndef create_headers(bearer_token):\n    headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n    return headers\n\n\ndef create_url(keyword, start_date, end_date, max_results = 10):\n    \n    search_url = \"https://api.twitter.com/2/tweets/search/all\" #Change to the endpoint you want to collect data from\n\n    #change params based on the endpoint you are using\n    query_params = {'query': keyword,\n                    'start_time': start_date,\n                    'end_time': end_date,\n                    'max_results': max_results,\n                    'expansions': 'author_id,in_reply_to_user_id,geo.place_id',\n                    'tweet.fields': 'id,text,author_id,in_reply_to_user_id,geo,conversation_id,created_at,lang,public_metrics,referenced_tweets,reply_settings,source',\n                    'user.fields': 'id,name,username,created_at,description,public_metrics,verified',\n                    'place.fields': 'full_name,id,country,country_code,geo,name,place_type',\n                    'next_token': {}}\n    return (search_url, query_params)\n\n\ndef connect_to_endpoint(url, headers, params, next_token = None):\n    params['next_token'] = next_token   #params object received from create_url function\n    response = requests.request(\"GET\", url, headers = headers, params = params)\n    print(\"Endpoint Response Code: \" + str(response.status_code))\n    if response.status_code != 200:\n        raise Exception(response.status_code, response.text)\n    return response.json()\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-08-22T21:25:14.050876Z","iopub.execute_input":"2022-08-22T21:25:14.051731Z","iopub.status.idle":"2022-08-22T21:25:14.064830Z","shell.execute_reply.started":"2022-08-22T21:25:14.051686Z","shell.execute_reply":"2022-08-22T21:25:14.063876Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def fetch_tweets(keyword, total_tweets, max_results, start_list, end_list):\n    # Create file\n    csvFile = open(keyword+\"_data.csv\", \"a\", newline=\"\", encoding='utf-8')\n    csvWriter = csv.writer(csvFile)\n\n    #Create headers for the data you want to save, in this example, we only want save these columns in our dataset\n    csvWriter.writerow(['author id', 'created_at', 'geo', 'id','lang', 'like_count', 'quote_count', 'reply_count','retweet_count','source','tweet'])\n    csvFile.close()\n\n    for i in range(0,len(start_list)):\n\n        # Inputs\n        count = 0 # Counting tweets per time period\n        max_count = 100 # Max tweets per time period\n        flag = True\n        next_token = None\n        \n        # Check if flag is true\n        while flag:\n            # Check if max_count reached\n            if count >= max_count:\n                break\n            print(\"-------------------\")\n            print(\"Token: \", next_token)\n            url = create_url(keyword, start_list[i],end_list[i], max_results)\n            json_response = connect_to_endpoint(url[0], headers, url[1], next_token)\n            result_count = json_response['meta']['result_count']\n\n            if 'next_token' in json_response['meta']:\n                # Save the token to use for next call\n                next_token = json_response['meta']['next_token']\n                print(\"Next Token: \", next_token)\n                if result_count is not None and result_count > 0 and next_token is not None:\n                    print(\"Start Date: \", start_list[i])\n                    append_to_csv(json_response, keyword+\"_data.csv\")\n                    count += result_count\n                    total_tweets += result_count\n                    print(\"Total # of Tweets added: \", total_tweets)\n                    print(\"-------------------\")\n                    time.sleep(5)                \n            # If no next token exists\n            else:\n                if result_count is not None and result_count > 0:\n                    print(\"-------------------\")\n                    print(\"Start Date: \", start_list[i])\n                    append_to_csv(json_response, keyword+\"_data.csv\")\n                    count += result_count\n                    total_tweets += result_count\n                    print(\"Total # of Tweets added: \", total_tweets)\n                    print(\"-------------------\")\n                    time.sleep(5)\n                \n                #Since this is the final request, turn flag to false to move to the next time period.\n                flag = False\n                next_token = None\n            time.sleep(5)\n        print(\"Total number of results: \", total_tweets)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-08-22T21:25:14.067900Z","iopub.execute_input":"2022-08-22T21:25:14.069039Z","iopub.status.idle":"2022-08-22T21:25:14.083264Z","shell.execute_reply.started":"2022-08-22T21:25:14.068979Z","shell.execute_reply":"2022-08-22T21:25:14.082169Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def append_to_csv(json_response, fileName):\n    \n\n    #A counter variable\n    counter = 0\n\n    #Open OR create the target CSV file\n    csvFile = open(fileName, \"a\", newline=\"\", encoding='utf-8')\n    csvWriter = csv.writer(csvFile)\n\n    #Loop through each tweet\n    for tweet in json_response['data']:\n        \n        # We will create a variable for each since some of the keys might not exist for some tweets\n        # So we will account for that\n\n        # 1. Author ID\n        author_id = tweet['author_id']\n\n        # 2. Time created\n        created_at = dateutil.parser.parse(tweet['created_at'])\n\n        # 3. Geolocation\n        if ('geo' in tweet):\n            if('place_id' in tweet['geo']):\n                geo = tweet['geo']['place_id']\n            else:\n                print(tweet['geo'])\n        else:\n            geo = \" \"\n\n        # 4. Tweet ID\n        tweet_id = tweet['id']\n\n        # 5. Language\n        lang = tweet['lang']\n\n        # 6. Tweet metrics\n        retweet_count = tweet['public_metrics']['retweet_count']\n        reply_count = tweet['public_metrics']['reply_count']\n        like_count = tweet['public_metrics']['like_count']\n        quote_count = tweet['public_metrics']['quote_count']\n\n        # 7. source\n        #source = tweet['source']\n\n        if ('source' in tweet):   \n            source= tweet['source']\n        else:\n            source = \" \"\n\n        # 8. Tweet text\n        text = tweet['text']\n        \n        # Assemble all data in a list\n        res = [author_id, created_at, geo, tweet_id, lang, like_count, quote_count, reply_count, retweet_count, source, text]\n        \n        # Append the result to the CSV file\n        csvWriter.writerow(res)\n        counter += 1\n\n    # When done, close the CSV file\n    csvFile.close()\n\n    # Print the number of tweets for this iteration\n    print(\"# of Tweets added from this response: \", counter) \n\n","metadata":{"execution":{"iopub.status.busy":"2022-08-22T21:25:14.084701Z","iopub.execute_input":"2022-08-22T21:25:14.085078Z","iopub.status.idle":"2022-08-22T21:25:14.100744Z","shell.execute_reply.started":"2022-08-22T21:25:14.085046Z","shell.execute_reply":"2022-08-22T21:25:14.099658Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"keywords =[\"with guilt\",\"saddled\", \"the guilt\" ,\"compunction\", \"misgiving\", \"prick\", \"qualm\", \"scruple\",\"fault\",\"liability\", \"rap\", \n           \"responsibility\",\"chagrin\", \"sorrow\",\"bloodguilt\", \"bloodguiltiness\",\"apology\", \"excuses\", \"hand-wringing\", \"mea culpa\",\"contriteness\",\n           \"contrition\", \"penitence\", \"regret\",\"embarrassment\",\"anguish\", \"distress\", \"grief\", \"ruth\", \"sadness\",\"remorsefulness\", \n           \"repentance\",\"blame\", \"culpability\",\" remorse\",\"rue\", \"self-reproach\", \"shame\",\"#feelingguilty\",\"I wish\",\"#GUILT\",\"#GUILTYthoughts\", \"#GUILTYconcience\", \"#mybad\",\"#iconfess\"]","metadata":{"execution":{"iopub.status.busy":"2022-08-22T21:25:14.128790Z","iopub.execute_input":"2022-08-22T21:25:14.129420Z","iopub.status.idle":"2022-08-22T21:25:14.135022Z","shell.execute_reply.started":"2022-08-22T21:25:14.129387Z","shell.execute_reply":"2022-08-22T21:25:14.133978Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"#Inputs for the request\nbearer_token = auth()\nheaders = create_headers(bearer_token)\n#keywords = [\"#feelingguilty\",\"I wish\",\"#GUILT\",\"#GUILTYthoughts\", \"#GUILTYconcience\", \"#mybad\",\"#iconfess\"]\n\n\n\nstart_list =    ['2011-01-01T00:00:00.000Z',\n                 '2012-01-01T00:00:00.000Z',\n                 '2013-01-01T00:00:00.000Z',\n                 '2014-01-01T00:00:00.000Z',\n                 '2015-01-01T00:00:00.000Z',\n                 '2016-01-01T00:00:00.000Z',\n                 '2017-01-01T00:00:00.000Z',\n                 '2018-01-01T00:00:00.000Z',\n                 '2019-01-01T00:00:00.000Z',\n                 '2020-01-01T00:00:00.000Z',\n                 '2021-01-01T00:00:00.000Z',]\n\nend_list =      ['2011-12-31T00:00:00.000Z',\n                 '2012-12-31T00:00:00.000Z',\n                 '2013-12-31T00:00:00.000Z',\n                 '2014-12-31T00:00:00.000Z',\n                 '2015-12-31T00:00:00.000Z',\n                 '2016-12-31T00:00:00.000Z',\n                 '2017-12-31T00:00:00.000Z',\n                 '2018-12-31T00:00:00.000Z',\n                 '2019-12-31T00:00:00.000Z',\n                 '2020-12-31T00:00:00.000Z',\n                 '2021-12-31T00:00:00.000Z',]\nmax_results = 500\n\n#Total number of tweets we collected from the loop\ntotal_tweets = 0\n\nfor x in keywords:\n    fetch_tweets(x,total_tweets,500, start_list, end_list)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n\n\nimport pandas as pd\nimport glob \nimport os \nimport sys\n# setting the path for joining multiple files \nfiles = os.path.join(sys.path[0], \"*.csv\") \n# list of merged files returned \nfiles = glob.glob(files) \nprint(\"Resultant CSV after joining all CSV files at a particular location...\"); \n# joining files with concat and read_csv \ndf = pd.concat(map(pd.read_csv, files), ignore_index=True) \nprint(df)\n\ndf.to_csv('allguilttweets.csv', header=True, index=False, columns=list(df.axes[1]))\n","metadata":{"execution":{"iopub.status.busy":"2022-08-22T22:58:40.736503Z","iopub.execute_input":"2022-08-22T22:58:40.736833Z","iopub.status.idle":"2022-08-22T22:58:44.756314Z","shell.execute_reply.started":"2022-08-22T22:58:40.736804Z","shell.execute_reply":"2022-08-22T22:58:44.755122Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Resultant CSV after joining all CSV files at a particular location...\n                 author id                 created_at geo  \\\n0                134693758  2011-12-30 23:59:53+00:00       \n1                109984248  2011-12-30 23:59:38+00:00       \n2                 25256735  2011-12-30 23:59:36+00:00       \n3                206338082  2011-12-30 23:59:05+00:00       \n4                 37929470  2011-12-30 23:58:57+00:00       \n...                    ...                        ...  ..   \n302558  715095151442599936  2021-12-20 15:13:37+00:00       \n302559          1467345817  2021-12-20 14:58:57+00:00       \n302560          1473346147  2021-12-20 14:48:22+00:00       \n302561           588683095  2021-12-20 13:55:25+00:00       \n302562           282575696  2021-12-20 13:38:09+00:00       \n\n                         id lang like_count quote_count reply_count  \\\n0        152901751677255680   en          0           0           1   \n1        152901691501592578   en          0           0           0   \n2        152901682525769728   en          0           0           0   \n3        152901552825319424   en          0           0           0   \n4        152901516989181954   en          0           0           0   \n...                     ...  ...        ...         ...         ...   \n302558  1472948306938675204   en          0           0           0   \n302559  1472944618039918593   en          0           0           0   \n302560  1472941952270323712   en          1           0           0   \n302561  1472928629008920577   en          0           0           0   \n302562  1472924281763946499   en          1           0           0   \n\n       retweet_count               source  \\\n0                  0   Twitter for iPhone   \n1                  0   Twitter for iPhone   \n2                 55              Seesmic   \n3                  0   Twitter for iPhone   \n4                  0           Foursquare   \n...              ...                  ...   \n302558             0      Twitter Web App   \n302559           102   Twitter for iPhone   \n302560             0      Twitter Web App   \n302561           102  Twitter for Android   \n302562             0     Twitter for iPad   \n\n                                                    tweet  \n0       @ruthscott2fm Hey Ruth. Is it any good?\\n#bikr...  \n1       Funny thing about it me and my ex the only peo...  \n2       RT @Ruth_A_Buzzi: Don't you hate it when you'r...  \n3       @RuthGorman_UTV Ruth have u give big Paddy Van...  \n4       I'm at Ruth's Chris Steak House (1700 N West S...  \n...                                                   ...  \n302558  @michaelsobolik Because many that are \"conserv...  \n302559  RT @VivianBercovici: @mindingottawa @FinanceCa...  \n302560  I'm starting a counseling practice for people ...  \n302561  RT @VivianBercovici: @mindingottawa @FinanceCa...  \n302562  @chloelou_k ...who has no compunction in libel...  \n\n[302563 rows x 11 columns]\n","output_type":"stream"}]},{"cell_type":"code","source":"extension = 'csv' \nall_filenames = [i for i in glob.glob('./*.{}'.format(extension))]\n\nappended_data = []\nfor infile in all_filenames:\n    data = pd.read_csv(infile, engine='python')\n    # store DataFrame in list\n    appended_data.append(data)\n# see pd.concat documentation for more info\nappended_data = pd.concat(appended_data)\n# write DataFrame to an excel sheet \nappended_data.to_csv( \"combined_cleaned_raw_keywords_twitter.csv\", index=False, encoding='utf-8-sig')\n","metadata":{"execution":{"iopub.status.busy":"2022-08-22T23:35:47.295675Z","iopub.execute_input":"2022-08-22T23:35:47.296464Z","iopub.status.idle":"2022-08-22T23:35:59.058631Z","shell.execute_reply.started":"2022-08-22T23:35:47.296426Z","shell.execute_reply":"2022-08-22T23:35:59.057685Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"appended_data.drop_duplicates(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-08-22T23:55:02.587568Z","iopub.execute_input":"2022-08-22T23:55:02.588019Z","iopub.status.idle":"2022-08-22T23:55:03.912436Z","shell.execute_reply.started":"2022-08-22T23:55:02.587981Z","shell.execute_reply":"2022-08-22T23:55:03.911209Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"appended_data.shape","metadata":{"execution":{"iopub.status.busy":"2022-08-22T23:55:11.037558Z","iopub.execute_input":"2022-08-22T23:55:11.037991Z","iopub.status.idle":"2022-08-22T23:55:11.045744Z","shell.execute_reply.started":"2022-08-22T23:55:11.037949Z","shell.execute_reply":"2022-08-22T23:55:11.044342Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"(433320, 11)"},"metadata":{}}]}]}