{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n!pip install psaw\n#import praw\n#from praw.models  import MoreComments\n\nfrom psaw import PushshiftAPI\nfrom datetime import datetime as dt\n\n\nimport glob\n\n\npd.set_option('max_colwidth', 500)\npd.set_option('max_columns', 50)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"reddit = praw.Reddit(\n    client_id='CO7FMREhxmYQEmYL9_qmZw', \n    client_secret='R0OqI7MB8QNnHvzo0lMvnCohfvGi4Q', \n    user_agent='my_user_agent')\"\"\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"keywords =[\"with guilt\",\"saddled\", \"the guilt\" ,\"compunction\", \"misgiving\", \"prick\", \"qualm\", \"scruple\"]\n\nkeywords2=[\"fault\",\"liability\", \"rap\", \"responsibility\",\"chagrin\", \"sorrow\",\"bloodguilt\", \"bloodguiltiness\"]\n\nkeywords3=[\"apology\", \"excuses\", \"hand-wringing\", \"mea culpa\",\"contriteness\", \"contrition\", \"penitence\", \"regret\"]\nkeywords4=[\"embarrassment\",\"anguish\", \"distress\", \"grief\", \"ruth\", \"sadness\",\"remorsefulness\", \"repentance\"]\nkeywords5=[\"blame\", \"culpability\",\" remorse\",\"rue\", \"self-reproach\", \"shame\"]\n\nsubreddits=['confessions']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"api = PushshiftAPI()\n\n \n\n\nfor keyword in subreddits:\n    start_epoch=int(dt(2012, 1, 1).timestamp()) #start_date\n    end_epoch=int(dt(2022, 1, 31).timestamp()) #end_date \n    while start_epoch < end_epoch + 86200:    \n        api_request_generator =  api.search_submissions(subreddit=keyword, after = start_epoch, before = start_epoch + 8620000) \n    # You can change the name of the Subreddit \n    # 86200 * n // n = duration of collection  (here, duration is 10 days)\n        aita_submissions = pd.DataFrame([submission.d_ for submission in api_request_generator])\n        \n        if aita_submissions.empty:\n            print('DataFrame is empty!')\n        else:\n            \n            aita_submissions['date'] = pd.to_datetime(aita_submissions['created_utc'], utc=True, unit='s')\n            aita_central = aita_submissions[['author', 'date', 'title', 'selftext', 'permalink', 'subreddit', 'score', 'num_comments' ]]\n\n            start_date = dt.fromtimestamp(start_epoch)\n            dateStr = start_date.strftime(\"%Y %b %d\")\n            print(dateStr)\n            \n            aita_central.to_csv(r\"sub_\" + keyword+dateStr + \".csv\", index = False, header = True)\n        \n        start_epoch += 8600000 # 86000 * duration of collection \"\"\"\n\n\nstart_epoch=int(dt(2012, 3, 1).timestamp()) #start_date\nend_epoch=int(dt(2022, 1, 31).timestamp()) #end_date ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"extension = 'csv' \nall_filenames = [i for i in glob.glob('*.{}'.format(extension))]\n\n#combine all files in the list\ncombined_csv = pd.concat([pd.read_csv(f) for f in all_filenames ]) \n#export to csv\ncombined_csv.to_csv( \"combinedfkey5_csv.csv\", index=False, encoding='utf-8-sig')","metadata":{},"execution_count":null,"outputs":[]}]}